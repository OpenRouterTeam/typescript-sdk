/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import { chatComplete, CompleteAcceptEnum } from "../funcs/chatComplete.js";
import { ClientSDK, RequestOptions } from "../lib/sdks.js";
import * as models from "../models/index.js";
import * as operations from "../models/operations/index.js";
import { unwrapAsync } from "../types/fp.js";
// #region imports
import { EventStream } from "../lib/event-streams.js";
// #endregion imports

export { CompleteAcceptEnum } from "../funcs/chatComplete.js";

export class Chat extends ClientSDK {
  // #region sdk-class-body
  /**
   * Create a chat completion without streaming (the default)
   * @param request - Request with stream: false/undefined
   * @param options - Request options
   * @returns ChatCompletion response
   */
  async complete(
    request: models.ChatCompletionCreateParams & {
      stream?: false | null | undefined;
    },
    options?: RequestOptions,
  ): Promise<models.ChatCompletion>;

  /**
   * Create a chat completion with streaming enabled
   * @param request - Request with stream: true
   * @param options - Request options
   * @returns EventStream for streaming responses
   */
  async complete(
    request: models.ChatCompletionCreateParams & { stream: true },
    options?: RequestOptions,
  ): Promise<EventStream<models.ChatCompletionChunk>>;
  // #endregion sdk-class-body

  /**
   * Create a chat completion
   *
   * @remarks
   * Creates a model response for the given chat conversation. Supports both streaming and non-streaming modes.
   */
  async complete(
    request?: models.ChatCompletionCreateParams | undefined,
    options?: RequestOptions & { acceptHeaderOverride?: CompleteAcceptEnum },
  ): Promise<operations.PostChatCompletionsResponse> {
    return unwrapAsync(chatComplete(
      this,
      request,
      options,
    ));
  }
}
